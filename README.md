# Shutka V2: Ultra-Efficient TypeScript Coding Agent

Shutka is a state-of-the-art **VL-JEPA** (Vision-Language Joint Embedding Predictive Architecture) model specifically optimized for low-latency, high-precision TypeScript code assistance on local hardware.

## ğŸš€ Key Innovations (V2)

Shutka V2 incorporates cutting-edge architectural advancements to provide a premium coding experience:

- **VL-JEPA Paradigm**: Non-autoregressive representation learning using semantic patches.
- **Efficient Tokenizer (cl100k_base)**: Upgraded to 100k vocabulary for superior code compression and GPT-4 compatibility.
- **RMSNorm & SwiGLU**: Modern, hardware-efficient architecture components.
- **HTTP/2 Support**: Optimized API server for ultra-low latency streaming.
- **Dynamic FAISS Memory (RAG)**: Mutable memory bank with live API-based context injection.

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ models/
â”‚   â””â”€â”€ shutka.py             # Shutka V2 (RMSNorm, RoPE, SwiGLU, BitLinear)
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ trainer.py            # Phase-aware trainer with GPU optimizations
â”‚   â”œâ”€â”€ train_typescript.py   # PHASE 1: Syntax & Structure training script
â”‚   â”œâ”€â”€ train_real_data.py    # PHASE 2: Instruction-following training script
â”‚   â”œâ”€â”€ typescript_loader.py  # Rich semantic extractor (classes, types, etc.)
â”‚   â””â”€â”€ real_instruction_loader.py # Multi-source instruction streamer
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ eval.py               # Main evaluation entry point
â”‚   â”œâ”€â”€ evaluator.py          # Representation & Retrieval metrics
â”‚   â”œâ”€â”€ test_syntax.py        # Bun-powered TS syntax verification
â”‚   â”œâ”€â”€ test_programming.py   # Functional logic verification
â”‚   â”œâ”€â”€ test_algorithmic.py   # Complex algorithmic tests
â”‚   â””â”€â”€ test_suites/          # JSON definitions for all tests
â”œâ”€â”€ config.py                 # Unified hyperparameter management
â”œâ”€â”€ evaluate_shutka.py        # Wrapper for evaluation runs
â”œâ”€â”€ KAGGLE_GUIDE.md           # Cloud training blueprints
â””â”€â”€ README.md                 # Project documentation
```

## ğŸ› ï¸ Dynamic Memory Management

Shutka V2 features a mutable FAISS memory bank. You can manage the model's knowledge without retraining:

```python
from models.shutka import UltraEfficientTextJEPA

model = UltraEfficientTextJEPA()
bank = model.predictor.memory_bank

# 1. Add new knowledge
ids = bank.add_memory(new_embeddings, ["Updated API documentation..."])

# 2. Delete stale info
bank.delete_memory(ids)

# 3. Update existing entry
bank.update_memory(old_id, new_embeddings, "New implementation...")
```

## ğŸš€ Getting Started

### 1. Installation

## for CPU

```bash
# Optimized for Bun & Python 3.10+
pip install faiss-cpu datasets tiktoken torch numpy tqdm hypercorn h2
```

#### for GPU

```bash
# Optimized for Bun & Python 3.10+
pip install faiss-gpu datasets tiktoken torch numpy tqdm bitsandbytes fastapi hypercorn h2
```

### 2. OpenAI-Compatible API

Expose Shutka as a local server for **Cursor**, **VS Code**, or **Continue**:

```bash
python api_server.py
```

#### Recommended Configuration (continue)

configure continue vscode extension settings:

```yaml
name: Local Config
version: 1.0.0
schema: v1

models:
  - name: Shutka Local
    provider: openai
    model: shutka-v2
    apiBase: http://localhost:8000/v1
    apiKey: anything
```

### 4. Dynamic Context Injection

You can "teach" Shutka new information on the fly using the specialized memory endpoint:

```bash
curl -X POST http://localhost:8000/v1/memory \
     -H "Content-Type: application/json" \
     -d '{"text": "The new project structure uses Bun for execution..."}'
```

### 3. Recommended Training Sequence

To turn Shutka into a premium coding agent, we recommend a two-phase training approach:

#### Phase 1: Syntax (The Grammar)

Learn the syntax and structural patterns of TypeScript.

```bash
python training/train_typescript.py --max_samples 50000 --epochs 5
```

#### Phase 2: Instruction Following (The Agent)

Train the model to map natural language to code using real-world data.

```bash
python training/train_real_data.py --resume checkpoints/best_model.pt --epochs 10
```

### 4. Evaluation (Bun Optimized)

```bash
# Verify syntax and programming logic
python evaluation/eval.py --checkpoint checkpoints/best_model.pt
```

## ğŸ“Š Performance

Shutka V2 is designed to run on a **GTX 1050 (4GB)** or even purely on **CPUs** while maintaining high accuracy, achieving a ~2.0GB memory footprint in full training mode and <1GB during inference.

## ğŸ“„ References

- [VL-JEPA (Joint Embedding Predictive Architecture)](https://arxiv.org/abs/2512.10942)
- [RoPE (Rotary Positional Embeddings)](https://arxiv.org/abs/2104.09864)
- [Llama-3 Architecture (RMSNorm & SwiGLU)](https://ai.meta.com/blog/meta-llama-3/)
